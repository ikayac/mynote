# çµ±è¨ˆçš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã®è©³ç´°èª¬æ˜

## ğŸ“Š æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€å¼·åº¦åˆ†å¸ƒã®çµ±è¨ˆçš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ä½¿ç”¨ã•ã‚Œã‚‹æ‰‹æ³•ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ”¬ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ‰‹æ³•

### æ··åˆåˆ‡æ–­æ­£è¦åˆ†å¸ƒã®ç”Ÿæˆ

#### 1. åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```python
# ç¬¬1æˆåˆ†ï¼ˆ270MPaä»˜è¿‘ï¼‰
mu1, sigma1 = 270, 15
# ç¬¬2æˆåˆ†ï¼ˆ300MPaä»˜è¿‘ï¼‰  
mu2, sigma2 = 300, 12
# æ··åˆæ¯”ç‡
weights = [0.6, 0.4]
# åˆ‡æ–­ç‚¹
truncation_point = 235
```

#### 2. Box-Mullerå¤‰æ›
æ­£è¦åˆ†å¸ƒã®ä¹±æ•°ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€Box-Mullerå¤‰æ›ã‚’ä½¿ç”¨ï¼š

```python
def box_muller(mu, sigma):
    u1 = random.random()  # ä¸€æ§˜ä¹±æ•°
    u2 = random.random()  # ä¸€æ§˜ä¹±æ•°
    
    # Box-Mullerå¤‰æ›
    z0 = math.sqrt(-2 * math.log(u1)) * math.cos(2 * math.pi * u2)
    
    return mu + sigma * z0
```

#### 3. åˆ‡æ–­å‡¦ç†
åˆ‡æ–­ç‚¹ä»¥ä¸‹ã®å€¤ã‚’é™¤å¤–ã—ã€é©åˆ‡ãªã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç¢ºä¿ï¼š

```python
# ç¬¬1æˆåˆ†ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
samples1 = []
while len(samples1) < n1:
    sample = box_muller(mu1, sigma1)
    if sample >= truncation_point:
        samples1.append(sample)
```

## ğŸ“ˆ åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ‰‹æ³•

### 1. å˜ä¸€åˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°

#### æ­£è¦åˆ†å¸ƒ
```python
# æœ€å°¤æ¨å®šã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
mu = np.mean(data)
sigma = np.std(data)
```

#### ãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒ
```python
# scipy.stats.weibull_min.fit()ã‚’ä½¿ç”¨
shape, loc, scale = stats.weibull_min.fit(data)
```

#### å¯¾æ•°æ­£è¦åˆ†å¸ƒ
```python
# å¯¾æ•°å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ã§æ­£è¦åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
log_data = np.log(data)
mu_log = np.mean(log_data)
sigma_log = np.std(log_data)
```

#### ã‚¬ãƒ³ãƒåˆ†å¸ƒ
```python
# ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ³•ã«ã‚ˆã‚‹åˆæœŸå€¤æ¨å®š
if std > 0:
    shape_gamma = (mean - min_val) ** 2 / (std ** 2)
    scale_gamma = (std ** 2) / (mean - min_val)
```

### 2. æ··åˆåˆ†å¸ƒã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°

#### æ··åˆæ­£è¦åˆ†å¸ƒ
```python
# GaussianMixtureã‚’ä½¿ç”¨
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(data.reshape(-1, 1))

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŠ½å‡º
mus = gmm.means_.flatten()
sigmas = np.sqrt(gmm.covariances_.flatten())
weights = gmm.weights_
```

#### æ··åˆåˆ‡æ–­æ­£è¦åˆ†å¸ƒ
```python
# ç°¡æ˜“çš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹åˆæœŸå€¤æ¨å®š
sorted_data = np.sort(data)
n = len(data)
group_size = n // 2

group1 = sorted_data[:group_size]
group2 = sorted_data[group_size:]

mu1, sigma1 = np.mean(group1), np.std(group1)
mu2, sigma2 = np.mean(group2), np.std(group2)
weight1, weight2 = len(group1) / n, len(group2) / n
```

## ğŸ“Š çµ±è¨ˆçš„è©•ä¾¡æ‰‹æ³•

### 1. å¯¾æ•°å°¤åº¦ã®è¨ˆç®—

#### æ­£è¦åˆ†å¸ƒ
```python
def normal_log_likelihood(data, mu, sigma):
    return np.sum(stats.norm.logpdf(data, mu, sigma))
```

#### åˆ‡æ–­æ­£è¦åˆ†å¸ƒ
```python
def truncated_normal_log_likelihood(data, mu, sigma, truncation_point):
    # åˆ‡æ–­æ­£è¦åˆ†å¸ƒã®å¯¾æ•°å°¤åº¦
    log_pdf = stats.norm.logpdf(data, mu, sigma)
    log_cdf_trunc = stats.norm.logcdf(truncation_point, mu, sigma)
    log_surv_trunc = np.log(1 - np.exp(log_cdf_trunc))
    
    return np.sum(log_pdf - log_surv_trunc)
```

#### æ··åˆåˆ†å¸ƒ
```python
def mixture_log_likelihood(data, components, weights):
    log_likelihood = 0
    for i, component in enumerate(components):
        log_likelihood += weights[i] * component.pdf(data)
    
    return np.sum(np.log(log_likelihood + 1e-10))
```

### 2. AICã®è¨ˆç®—

```python
def calculate_aic(log_likelihood, n_params, n_samples):
    """
    Akaike Information Criterion (AIC)ã®è¨ˆç®—
    
    AIC = 2k - 2ln(L)
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
    - log_likelihood: å¯¾æ•°å°¤åº¦
    - n_params: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
    - n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
    
    æˆ»ã‚Šå€¤:
    - AICå€¤ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
    """
    return 2 * n_params - 2 * log_likelihood
```

### 3. 5%åˆ†ä½ç‚¹ã®è¨ˆç®—

#### æ­£è¦åˆ†å¸ƒ
```python
def normal_5th_percentile(mu, sigma):
    return stats.norm.ppf(0.05, mu, sigma)
```

#### åˆ‡æ–­æ­£è¦åˆ†å¸ƒ
```python
def truncated_normal_5th_percentile(mu, sigma, truncation_point):
    # åˆ‡æ–­ç‚¹ä»¥ä¸‹ã®ç¢ºç‡ã‚’è€ƒæ…®ã—ã¦èª¿æ•´
    z = (truncation_point - mu) / sigma
    if z < 10:
        norm_factor = 1 - stats.norm.cdf(truncation_point, mu, sigma)
        if norm_factor > 1e-10:
            p_adjusted = 0.05 * norm_factor
            return stats.norm.ppf(p_adjusted, mu, sigma)
    
    return truncation_point
```

#### ãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒ
```python
def weibull_5th_percentile(shape, loc, scale):
    return loc + scale * ((-np.log(0.95)) ** (1 / shape))
```

## ğŸ¨ å¯è¦–åŒ–æ‰‹æ³•

### 1. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®ä½œæˆ

```python
def create_histogram(data, bins=20):
    """
    ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä½œæˆ
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
    - data: åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
    - bins: ãƒ“ãƒ³æ•°
    
    æˆ»ã‚Šå€¤:
    - bin_centers: ãƒ“ãƒ³ã®ä¸­å¿ƒå€¤
    - bin_counts: å„ãƒ“ãƒ³ã®åº¦æ•°
    """
    min_val = np.min(data)
    max_val = np.max(data)
    bin_width = (max_val - min_val) / bins
    
    bin_centers = []
    for i in range(bins):
        center = min_val + (i + 0.5) * bin_width
        bin_centers.append(center)
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®ä½œæˆ
    hist, _ = np.histogram(data, bins=bins)
    
    return bin_centers, hist
```

### 2. å¯†åº¦æ›²ç·šã®æç”»

```python
def plot_density_curves(x_range, models, results):
    """
    å„ãƒ¢ãƒ‡ãƒ«ã®å¯†åº¦æ›²ç·šã‚’æç”»
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
    - x_range: xè»¸ã®ç¯„å›²
    - models: ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    - results: åˆ†æçµæœ
    """
    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
    
    for i, (name, model) in enumerate(models.items()):
        if name in results:
            try:
                # å„ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸå¯†åº¦æ›²ç·šã®æç”»
                if name == 'single_normal':
                    mu, sigma = model['params']['mu'], model['params']['sigma']
                    plt.plot(x_range, stats.norm.pdf(x_range, mu, sigma), 
                            color=colors[i], linewidth=2, label=name, alpha=0.8)
                
                # ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚‚åŒæ§˜ã«...
                
            except Exception as e:
                print(f"Warning: Could not plot {name}: {e}")
```

### 3. åŒ…æ‹¬çš„å¯è¦–åŒ–ã®æ§‹æˆ

```python
def create_comprehensive_visualization(data, all_models, results):
    """
    9ã¤ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã«ã‚ˆã‚‹åŒ…æ‹¬çš„å¯è¦–åŒ–
    
    ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆæ§‹æˆ:
    1. å…¨ä½“ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¨å¯†åº¦æ›²ç·šæ¯”è¼ƒ
    2. 270MPaä»˜è¿‘ã®è©³ç´°ï¼ˆç¬¬1ãƒ”ãƒ¼ã‚¯ï¼‰
    3. 300MPaä»˜è¿‘ã®è©³ç´°ï¼ˆç¬¬2ãƒ”ãƒ¼ã‚¯ï¼‰
    4. ä¸‹å´å°¾éƒ¨ã®è©³ç´°ï¼ˆ235-250MPaï¼‰
    5. AICã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    6. 5%åˆ†ä½ç‚¹ã®æ¯”è¼ƒ
    7. å¯¾æ•°å°¤åº¦ã®æ¯”è¼ƒ
    8. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ
    9. ãƒ•ã‚£ãƒƒãƒˆå…·åˆã®ç·åˆè©•ä¾¡
    """
    fig = plt.figure(figsize=(20, 15))
    
    # å„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ
    # ... (è©³ç´°ãªå®Ÿè£…)
    
    plt.tight_layout()
    plt.savefig('comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig
```

## ğŸ” æ•°å€¤çš„å®‰å®šæ€§ã®è€ƒæ…®

### 1. å¯¾æ•°å°¤åº¦è¨ˆç®—ã§ã®æ•°å€¤çš„å®‰å®šæ€§

```python
def stable_log_likelihood(data, model_name, params):
    """
    æ•°å€¤çš„ã«å®‰å®šã—ãŸå¯¾æ•°å°¤åº¦ã®è¨ˆç®—
    
    å•é¡Œç‚¹:
    - log(0) = -âˆ
    - éå¸¸ã«å°ã•ã„ç¢ºç‡ã§ã®æ•°å€¤çš„ä¸å®‰å®šæ€§
    
    å¯¾ç­–:
    - å°ã•ãªå€¤ã®è¿½åŠ  (1e-10)
    - é©åˆ‡ãªç¯„å›²ãƒã‚§ãƒƒã‚¯
    """
    try:
        if model_name == 'mixture_normal':
            mus, sigmas, weights = params['mus'], params['sigmas'], params['weights']
            
            log_likelihood = 0
            for i in range(len(mus)):
                if sigmas[i] > 0:
                    component_likelihood = weights[i] * stats.norm.pdf(data, mus[i], sigmas[i])
                    log_likelihood += component_likelihood
            
            # æ•°å€¤çš„å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤ã®è¿½åŠ 
            return np.sum(np.log(log_likelihood + 1e-10))
            
    except Exception as e:
        print(f"Error in log likelihood calculation: {e}")
        return -np.inf
```

### 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯

```python
def validate_parameters(params, model_name):
    """
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    
    ãƒã‚§ãƒƒã‚¯é …ç›®:
    - åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ > 0
    - å½¢çŠ¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ > 0
    - é‡ã¿ã®åˆè¨ˆ = 1
    """
    if model_name == 'single_normal':
        mu, sigma = params['mu'], params['sigma']
        if sigma <= 0:
            return False
    
    elif model_name == 'weibull':
        shape, loc, scale = params['shape'], params['loc'], params['scale']
        if shape <= 0 or scale <= 0:
            return False
    
    elif model_name == 'mixture_normal':
        mus, sigmas, weights = params['mus'], params['sigmas'], params['weights']
        if any(s <= 0 for s in sigmas):
            return False
        if abs(sum(weights) - 1.0) > 1e-6:
            return False
    
    return True
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Akaike, H. (1974). "A new look at the statistical model identification". IEEE Transactions on Automatic Control, 19(6), 716-723.

2. McLachlan, G., & Peel, D. (2000). "Finite Mixture Models". Wiley Series in Probability and Statistics.

3. Box, G. E. P., & Muller, M. E. (1958). "A Note on the Generation of Random Normal Deviates". The Annals of Mathematical Statistics, 29(2), 610-611.

4. Johnson, N. L., Kotz, S., & Balakrishnan, N. (1994). "Continuous Univariate Distributions, Volume 1". Wiley Series in Probability and Statistics.

## ğŸ”„ ä»Šå¾Œã®æ”¹å–„ç‚¹

1. **æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„**: ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
2. **ãƒ™ã‚¤ã‚ºæ¨å®šã®å°å…¥**: ä¸ç¢ºå®Ÿæ€§ã®å®šé‡åŒ–
3. **äº¤å·®æ¤œè¨¼**: ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã®è©•ä¾¡
4. **è‡ªå‹•ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•é¸æŠ
5. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªåˆ†æç’°å¢ƒ